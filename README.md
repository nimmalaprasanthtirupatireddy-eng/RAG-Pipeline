# RAG-Pipeline ğŸ¤–

# ğŸ“„ Retrieval-Augmented Generation (RAG) Application
### (Powered by Ollama â€“ Gemma2:2B)

This project implements a **Retrieval-Augmented Generation (RAG)** system using a **locally running LLM (Gemma2:2B via Ollama)**.
It combines **document retrieval** with **context-aware generation** to answer user queries accurately â€” without relying on cloud-based APIs.

The project now features a **state-of-the-art 3D Glassmorphism UI** for a premium user experience.

---

## ğŸš€ Features

-   **ğŸ–¥ï¸ Modern Web UI**: Premium 3D Glassmorphism design with interactive animations.
-   **ğŸ“š Multi-File Upload**: Drag & drop or select multiple PDFs at once.
-   **ğŸ“‚ Metadata Tracking**: Vector store tracks source filenames for accurate citation.
-   **ğŸ’¾ Persistent Storage**: FAISS vector database saves locally.
-   **ğŸ” Semantic Search**: Uses `sentence-transformers/all-MiniLM-L6-v2` embeddings.
-   **ğŸ¤– Local LLM**: Inference using **Ollama (Gemma2:2B)**.
-   **âš¡ FastAPI Backend**: High-performance API for all RAG operations.
-   **ğŸ” Fully Local**: Complete offline privacy and security.

---

## ğŸ› ï¸ Tech Stack

-   **Frontend**: HTML5, CSS3 (Glassmorphism), JavaScript (Vanilla), 3D Tilt Effects
-   **Backend**: FastAPI (Python)
-   **LLM Framework**: LangChain
-   **LLM**: Ollama â€“ `gemma2:2b`
-   **Vector Store**: FAISS
-   **Embeddings**: HuggingFace (`all-MiniLM-L6-v2`)

---

## ğŸ“‚ Project Structure

```
RAG-Pipeline/
â”‚
â”œâ”€â”€ api.py               # FastAPI Backend
â”œâ”€â”€ main.py              # CLI Entry Point
â”œâ”€â”€ serve_frontend.py    # Simple Python Web Server for UI
â”œâ”€â”€ requirements.txt     # Python Dependencies
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ simple_rag.py    # Core RAG Logic
â”‚   â”œâ”€â”€ vector_store.py  # FAISS Vector Store Management
â”‚   â”œâ”€â”€ pdf_reader.py    # PDF Text Extraction
â”‚   â””â”€â”€ ...
â””â”€â”€ frontend/            # Modern Web UI
    â”œâ”€â”€ index.html
    â”œâ”€â”€ style.css
    â””â”€â”€ script.js
```

---

## âš™ï¸ Prerequisites

### 1ï¸âƒ£ Install Ollama
Download and install Ollama from: https://ollama.com

Verify installation:
```bash
ollama --version
ollama pull gemma2:2b
ollama run gemma2:2b
```

### 2ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/nimmalaprasanthtirupatireddy-eng/RAG-Pipeline.git
cd RAG-Pipeline
```

### 3ï¸âƒ£ Create Virtual Environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Mac/Linux
python3 -m venv venv
source venv/bin/activate
```

### 4ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Run the Application

### 1ï¸âƒ£ Start Ollama Server
Ollama usually runs automatically. If needed:
```bash
ollama serve
```

### 2ï¸âƒ£ Start FastAPI Backend
Open a terminal and run:
```bash
uvicorn api:app --reload
```
API Docs: http://127.0.0.1:8000/docs

### 3ï¸âƒ£ Start Modern UI (Recommended)
Open a **new terminal** and run:
```bash
python serve_frontend.py
```
ğŸ‘‰ Access the App: http://localhost:8080

*(Optional)* Legacy Streamlit UI:
```bash
streamlit run app.py
```

---

## ğŸ§  RAG Pipeline Flow

1.  **Upload**: User uploads multiple PDFs via the UI.
2.  **Process**: Backend reads and splits text into chunks.
3.  **Embed**: Generates embeddings for each chunk (with metadata).
4.  **Store**: Saves embeddings to local FAISS index.
5.  **Query**: User asks a question in the chat.
6.  **Retrieve**: System finds top-k relevant chunks.
7.  **Generate**: LLM (Gemma2) answers using the retrieved context.

---

## ğŸ“Œ Recent Updates

-   âœ… **UI Overhaul**: Implemented Gemini-style input bar and document drawer.
-   âœ… **Multi-File Support**: Upload and process batches of PDFs.
-   âœ… **Interactive 3D**: Added tilt effects and 3D welcome animations.
-   âœ… **Performance**: Optimized text chunking and storage.

---

## ğŸ“Œ Future Enhancements

-   ğŸ”„ Multi-model switching (Llama3, Mistral) via UI
-   ğŸ” User authentication system
-   ğŸ“Š Query analytics dashboard
-   ğŸ•¸ï¸ Web scraping for dynamic context

---

## ğŸ¤ Contributing

Contributions are welcome! Fork the repository and submit a pull request.

## â­ Acknowledgements

-   Ollama
-   Google Gemma
-   LangChain
-   FastAPI

---

### Created by & Contact

**Name**: Nimmala Prasanth Tirupati Reddy
**Email**: nimmalaprasanthtirupatireddy@gmail.com
**GitHub**: [nimmalaprasanthtirupatireddy-eng](https://github.com/nimmalaprasanthtirupatireddy-eng)
