# RAG-Pipeline ğŸ¤–

# ğŸ“„ Retrieval-Augmented Generation (RAG) Application
### (Powered by Ollama â€“ Gemma2:2B)

This project implements a **Retrieval-Augmented Generation (RAG)** system using a **locally running LLM (Gemma2:2B via Ollama)**.
It combines **document retrieval** with **context-aware generation** to answer user queries accurately â€” without relying on cloud-based APIs.

The project now features a **state-of-the-art 3D Glassmorphism UI** for a premium user experience.

---

## â“ Problem Statement
Large Language Models (LLMs) like GPT-4 or Gemini are powerful, but they have significant limitations:
*   **Hallucinations**: They can confidently generate incorrect information.
*   **Knowledge Cutoff**: They are unaware of events or data created after their training date.
*   **No Private Knowledge**: They cannot answer questions about your specific documents, emails, or internal databases.

## ğŸ’¡ The Solution: RAG Pipeline
This project bridges the gap using **Retrieval-Augmented Generation (RAG)**. Instead of relying solely on the LLM's pre-trained memory, the system:
1.  **Ingests** your private PDF documents.
2.  **Indexes** them into a searchable vector database.
3.  **Retrieves** the exact context relevant to your question.
4.  **Generates** an accurate answer based *only* on that context.

## ğŸš€ Why Use This Project?
*   **ğŸ”’ Complete Privacy**: Everything runs locally (**Ollama + FAISS**). Your sensitive documents never leave your machine.
*   **âœ… Truthful Answers**: The model answers based on your data, significantly reducing hallucinations.
*   **ğŸ’° Zero Cost**: Uses open-source models (Gemma 2) and local compute, avoiding expensive API tokens.
*   **ğŸ¨ Premium UI**: A polished, modern interface that makes interacting with your documents a delight.

---

## ğŸš€ Features

-   **ğŸ–¥ï¸ Modern Web UI**: Premium 3D Glassmorphism design with interactive animations.
-   **ğŸ“š Multi-File Upload**: Drag & drop or select multiple PDFs at once.
-   **ğŸ“‚ Metadata Tracking**: Vector store tracks source filenames for accurate citation.
-   **ğŸ’¾ Persistent Storage**: FAISS vector database saves locally.
-   **ğŸ” Semantic Search**: Uses `sentence-transformers/all-MiniLM-L6-v2` embeddings.
-   **ğŸ¤– Local LLM**: Inference using **Ollama (Gemma2:2B)**.
-   **âš¡ FastAPI Backend**: High-performance API for all RAG operations.
-   **ğŸ” Fully Local**: Complete offline privacy and security.

---

## ğŸ› ï¸ Tech Stack

-   **Frontend**: HTML5, CSS3 (Glassmorphism), JavaScript (Vanilla), 3D Tilt Effects
-   **Backend**: FastAPI (Python)
-   **LLM Framework**: LangChain
-   **LLM**: Ollama â€“ `gemma2:2b`
-   **Vector Store**: FAISS
-   **Embeddings**: HuggingFace (`all-MiniLM-L6-v2`)

---

## ğŸ“‚ Project Structure

```
RAG-Pipeline/
â”‚
â”œâ”€â”€ api.py               # FastAPI Backend
â”œâ”€â”€ main.py              # CLI Entry Point
â”œâ”€â”€ serve_frontend.py    # Simple Python Web Server for UI
â”œâ”€â”€ requirements.txt     # Python Dependencies
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ simple_rag.py    # Core RAG Logic
â”‚   â”œâ”€â”€ vector_store.py  # FAISS Vector Store Management
â”‚   â”œâ”€â”€ pdf_reader.py    # PDF Text Extraction
â”‚   â”œâ”€â”€ llm_chain.py     # LLM Chain & Prompt Engineering
â”‚   â”œâ”€â”€ text_splitter.py # Text Chunking Logic
â”‚   â””â”€â”€ logger.py        # Logging Configuration
â”œâ”€â”€ frontend/            # Modern Web UI
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”œâ”€â”€ extracted_text/      # Directory for debugging extracted text
â”œâ”€â”€ uploads/             # Directory for uploaded files
â””â”€â”€ faiss_db/            # Local Vector Store (FAISS)
```

---

## âš™ï¸ Prerequisites

### 1ï¸âƒ£ Install Ollama
Download and install Ollama from: https://ollama.com

Verify installation:
```bash
ollama --version
ollama pull gemma2:2b
ollama run gemma2:2b
```

### 2ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/nimmalaprasanthtirupatireddy-eng/RAG-Pipeline.git
cd RAG-Pipeline
```

### 3ï¸âƒ£ Create Virtual Environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Mac/Linux
python3 -m venv venv
source venv/bin/activate
```

### 4ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Run the Application

### 1ï¸âƒ£ Start Ollama Server
Ollama usually runs automatically. If needed:
```bash
ollama serve
```

### 2ï¸âƒ£ Start FastAPI Backend
Open a terminal and run:
```bash
uvicorn api:app --reload
```
API Docs: http://127.0.0.1:8000/docs

### 3ï¸âƒ£ Start Modern UI (Recommended)
Open a **new terminal** and run:
```bash
python serve_frontend.py
```
ğŸ‘‰ Access the App: http://localhost:8080

*(Optional)* Legacy Streamlit UI:
```bash
streamlit run app.py
```

---

## ğŸ§  RAG Pipeline Flow

1.  **Upload**: User uploads multiple PDFs via the UI.
2.  **Process**: Backend reads and splits text into chunks.
3.  **Embed**: Generates embeddings for each chunk (with metadata).
4.  **Store**: Saves embeddings to local FAISS index.
5.  **Query**: User asks a question in the chat.
6.  **Retrieve**: System finds top-k relevant chunks.
7.  **Generate**: LLM (Gemma2) answers using the retrieved context.

---

## ğŸ“Œ Recent Updates

-   âœ… **UI Overhaul**: Implemented Gemini-style input bar and document drawer.
-   âœ… **Multi-File Support**: Upload and process batches of PDFs.
-   âœ… **Prompt Engineering**: Enhanced LLM prompt for better table interpretation and data analysis.
-   âœ… **Raw Text Extraction**: Automatically extracts and saves processed PDF text to `extracted_text/combined.txt` for inspection.
-   âœ… **Interactive 3D**: Added tilt effects and 3D welcome animations.
-   âœ… **Performance**: Optimized text chunking and storage.

---

## ğŸ“Œ Future Enhancements

-   ğŸ”„ Multi-model switching (Llama3, Mistral) via UI
-   ğŸ” User authentication system
-   ğŸ“Š Query analytics dashboard
-   ğŸ•¸ï¸ Web scraping for dynamic context

---

## ğŸ¤ Contributing

Contributions are welcome! Fork the repository and submit a pull request.

## â­ Acknowledgements

-   Ollama
-   Google Gemma
-   LangChain
-   FastAPI

---

### Created by & Contact

**Name**: Nimmala Prasanth Tirupati Reddy
**Email**: nimmalaprasanthtirupatireddy@gmail.com
**GitHub**: [nimmalaprasanthtirupatireddy-eng](https://github.com/nimmalaprasanthtirupatireddy-eng)
