# RAG-Pipeline

# ğŸ“„ Retrieval-Augmented Generation (RAG) Application  
### (Powered by Ollama â€“ Gemma2:2B)

This project implements a **Retrieval-Augmented Generation (RAG)** system using a **locally running LLM (Gemma2:2B via Ollama)**.  
It combines **document retrieval** with **context-aware generation** to answer user queries accurately â€” without relying on cloud-based APIs.

---

## ğŸš€ Features

- ğŸ“š Document ingestion and text chunking
- ğŸ” Semantic search using vector embeddings
- ğŸ¤– Local LLM inference using **Ollama (Gemma2:2B)**
- âš¡ FastAPI backend for RAG APIs
- ğŸ–¥ï¸ Streamlit frontend for user interaction
- ğŸ”— LangChain for RAG pipeline orchestration
- ğŸ” Fully local & offline-friendly setup

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python  
- **Backend:** FastAPI  
- **Frontend:** Streamlit  
- **LLM Framework:** LangChain  
- **LLM:** Ollama â€“ `gemma2:2b`  
- **Vector Store:** FAISS / Chroma  
- **Embeddings:** Ollama / HuggingFace  

---

## ğŸ“‚ Project Structure

RAG-Pipeline/

â”‚

â”œâ”€â”€ api.py

â”œâ”€â”€ main.py

â”œâ”€â”€ app.py

â”œâ”€â”€ requirements.txt


---

## âš™ï¸ Prerequisites

### 1ï¸âƒ£ Install Ollama
Download and install Ollama from:

https://ollama.com


Verify installation:
```bash
ollama --version

ollama pull gemma2:2b

ollama run gemma2:2b
```

âš™ï¸ Installation

1ï¸âƒ£ Clone the repository

git clone https://github.com/nimmalaprasanthtirupatireddy-eng/RAG-Pipeline.git
cd RAG-Pipeline

2ï¸âƒ£ Create virtual environment

```bash
python -m venv venv

venv\Scripts\activate       
```

3ï¸âƒ£ Install dependencies

``` bash
pip install -r requirements.txt
```

â–¶ï¸ How to Run the Application

1ï¸âƒ£ Start Ollama server

Ollama usually runs automatically. If needed:

ollama serve

2ï¸âƒ£ Start FastAPI Backend
```bash
uvicorn api:app --reload
```
http://127.0.0.1:8000

3ï¸âƒ£ Start Streamlit Frontend

Open a new terminal:
```bash
streamlit run app.py
```
http://localhost:8501



ğŸ§  RAG Pipeline Flow

Load documents from /data/documents

Split text into chunks

Generate embeddings

Store embeddings in vector database

Retrieve top-k relevant chunks

Pass context + query to Gemma2:2B

Generate grounded response


ğŸ“Œ Advantages of Using Ollama + Gemma2

âœ… Fully local execution

âœ… No API cost

âœ… Better data privacy

âœ… Works offline

âœ… Ideal for personal & enterprise RAG systems


ğŸ“Œ Future Enhancements

ğŸ“„ Upload PDFs from UI

ğŸ—„ï¸ Persistent vector DB

ğŸ”„ Multi-model switching

ğŸ” User authentication

ğŸ“Š Query analytics

ğŸ¤ Contributing


Contributions are welcome!

Fork the repository and submit a pull request.


â­ Acknowledgements

Ollama

Google Gemma

LangChain

FastAPI

Streamlit


Created by & Contact 

Name : Nimmala Prasanth Tirupati Reddy

email : nimmalaprasanthtirupatireddy@gmail.com

github : https://github.com/nimmalaprasanthtirupatireddy-eng

